# HPC Cluster Monitoring Stack

# [QUICKSTART](https://git.sa1.test/gpu-cluster/monitoring/ansible-monitoring/-/blob/main/QUICKSTART.md)

## Features

- **üñ•Ô∏è Complete HPC stack monitoring**: System, GPU, Slurm, InfiniBand, IPMI
- **üè¢ Enterprise-ready**: HA setup with Pacemaker, custom Docker registry support
- **üéÆ GPU-optimized**: DCGM + custom process exporters with Slurm integration
- **üìä Long-term storage**: VictoriaMetrics integration for metrics retention
- **üî• Emergency response**: Automated cluster poweroff on critical temperature
- **üßπ Automated lifecycle**: Cleanup and maintenance scripts
- **üìà Rich visualization**: Pre-configured Grafana dashboards
- **üö® Intelligent alerting**: Temperature, hardware, network, and Slurm alerts

## üìä Supported Components

| Component | Target Nodes | Port | Description |
|-----------|--------------|------|-------------|
| **Node Exporter** | All nodes | 9100 | System metrics (CPU, RAM, disk, network) |
| **DCGM Exporter** | Compute nodes | 9400 | NVIDIA GPU metrics (temp, utilization, memory) |
| **Slurm Exporter** | Slurm master | 8080 | Job queue, node states, GPU accounting |
| **UFM Exporters** | UFM servers | 9001 | InfiniBand switch ports, link states |
| **IPMI Exporter** | Metrics server | 9290 | Hardware sensors (BMC, fans, power, temp) |
| **GPU Process Exporter** | Compute nodes | Timer | Detailed GPU process monitoring with Slurm jobs |
| **Prometheus** | Metrics server | 9090 | Metrics collection and alerting |
| **VictoriaMetrics** | Metrics server | 8428 | Long-term metrics storage (12 months default) |
| **Grafana** | Metrics server | 3000 | Visualization and dashboards |
| **Alertmanager** | Metrics server | 9093 | Alert routing (Telegram, email, webhooks) |

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Compute Nodes ‚îÇ    ‚îÇ   Slurm Master  ‚îÇ    ‚îÇ   UFM Servers   ‚îÇ
‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ
‚îÇ ‚Ä¢ Node Exporter ‚îÇ    ‚îÇ ‚Ä¢ Node Exporter ‚îÇ    ‚îÇ ‚Ä¢ Node Exporter ‚îÇ
‚îÇ ‚Ä¢ DCGM Exporter ‚îÇ    ‚îÇ ‚Ä¢ Slurm Export. ‚îÇ    ‚îÇ ‚Ä¢ IB Link Info  ‚îÇ
‚îÇ ‚Ä¢ GPU Proc Exp. ‚îÇ    ‚îÇ                 ‚îÇ    ‚îÇ ‚Ä¢ SM State Mon. ‚îÇ
‚îÇ ‚Ä¢ PCIe Monitor  ‚îÇ    ‚îÇ                 ‚îÇ    ‚îÇ ‚Ä¢ Cleanup Timer ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                       ‚îÇ                       ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ    Metrics Server       ‚îÇ
                    ‚îÇ                         ‚îÇ
                    ‚îÇ ‚Ä¢ Prometheus            ‚îÇ
                    ‚îÇ ‚Ä¢ VictoriaMetrics       ‚îÇ
                    ‚îÇ ‚Ä¢ Grafana               ‚îÇ
                    ‚îÇ ‚Ä¢ Alertmanager          ‚îÇ
                    ‚îÇ ‚Ä¢ IPMI Exporter         ‚îÇ
                    ‚îÇ ‚Ä¢ Emergency Poweroff    ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üöÄ Quick Start

### 1. Copy Example Inventory

```bash
cp roles/monitoring/tests/inventory inventories/my-cluster.yml
```

### 2. Customize for Your Environment

Edit `inventories/my-cluster.yml`:

```ini
[slurm-master]
slurm-master ansible_host=10.0.1.10

[compute]
cn[01:64] ansible_host_pattern=10.0.2.%d

[ufm]
ufm-cn1 ansible_host=10.0.3.1
ufm-cn2 ansible_host=10.0.3.2
ufm-sn1 ansible_host=10.0.4.1
ufm-sn2 ansible_host=10.0.4.2

[metrics]
metrics ansible_host=10.0.1.20

[all:vars]
cluster_name=my-hpc-cluster
# Add your custom variables here
```

### 3. Configure Secrets

Create `group_vars/all/vault.yml`:

```yaml
# Encrypt with: ansible-vault encrypt group_vars/all/vault.yml
vault_telegram_bot_token: "YOUR_BOT_TOKEN"
vault_telegram_chat_id: "YOUR_CHAT_ID"
vault_ipmi_password: "YOUR_IPMI_PASSWORD"
vault_cluster_poweroff_token: "YOUR_WEBHOOK_TOKEN"
vault_cluster_ipmi_password: "YOUR_CLUSTER_IPMI_PASSWORD"
```

### 4. Deploy

```bash
ansible-playbook -i inventories/my-cluster.yml site.yml
```

### 5. Access Dashboards

- **Grafana**: http://metrics-server:3000 (admin/admin)
- **Prometheus**: http://metrics-server:9090
- **VictoriaMetrics**: http://metrics-server:8428

## ‚öôÔ∏è Configuration

### Key Variables

Override these in your inventory or group_vars:

```yaml
# Cluster configuration
cluster_name: "production-hpc"
prometheus_retention: "30d"
victoriametrics_retention_period: "24"  # months

# Alert thresholds
cluster_poweroff_temp_threshold: 45  # ¬∞C
chip_hot_threshold: 85  # ¬∞C
disk_space_threshold: 85  # %

# Network ranges (for emergency poweroff)
cluster_datanet_prefix: "10.0.2"
cluster_ipminet_prefix: "10.0.1"
cluster_data_start: 1
cluster_data_end: 64

# Telegram notifications
telegram_enabled: true
telegram_bot_token: "{{ vault_telegram_bot_token }}"
telegram_chat_id: "{{ vault_telegram_chat_id }}"

# Feature toggles
victoriametrics_enabled: true
cluster_poweroff_enabled: true
health_checks_enabled: true
```

### Advanced Configuration

See [`defaults/main.yml`](defaults/main.yml) for all available variables (100+ options).

## üî• Emergency Features

### Automated Cluster Poweroff

When datacenter temperature exceeds threshold:

1. **Prometheus** detects high inlet temperature (>45¬∞C for >1 hour)
2. **Alertmanager** triggers emergency webhook
3. **Python webhook** executes graceful shutdown:
   - SSH poweroff to all compute nodes
   - SFA storage cluster shutdown
   - IPMI force poweroff if needed

**Safety**: Lock file prevents multiple executions, detailed logging.

### Critical Alerts

- **üå°Ô∏è Temperature**: Inlet temp, chip overheating, cooling failures
- **‚ö° Power**: PSU failures, voltage issues, chassis power
- **üîå Network**: Interface down, InfiniBand link failures
- **üíæ Storage**: Disk space, read-only filesystems, Lustre issues
- **üñ•Ô∏è Hardware**: RAM capacity, IPMI connectivity, fan failures
- **üìä Slurm**: Node failures, job queue issues

## üìÅ Directory Structure

```
monitoring/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ defaults/main.yml          # 100+ configurable variables
‚îú‚îÄ‚îÄ files/
‚îÇ   ‚îú‚îÄ‚îÄ dcgm-custom-metrics.csv
‚îÇ   ‚îú‚îÄ‚îÄ gpu-process-exporter.sh
‚îÇ   ‚îú‚îÄ‚îÄ ib_sw_exp.py
‚îÇ   ‚îú‚îÄ‚îÄ check_sm_state.sh
‚îÇ   ‚îî‚îÄ‚îÄ cleanup-metrics.sh
‚îú‚îÄ‚îÄ handlers/main.yml          # Service restart handlers
‚îú‚îÄ‚îÄ meta/main.yml              # Role metadata
‚îú‚îÄ‚îÄ tasks/
‚îÇ   ‚îú‚îÄ‚îÄ main.yml               # Main orchestration with health checks
‚îÇ   ‚îú‚îÄ‚îÄ install_node.yml       # Node exporter setup
‚îÇ   ‚îú‚îÄ‚îÄ install_dcgm.yml       # GPU monitoring setup
‚îÇ   ‚îú‚îÄ‚îÄ install_slurm.yml      # Slurm metrics setup
‚îÇ   ‚îú‚îÄ‚îÄ install_ufm.yml        # InfiniBand monitoring setup
‚îÇ   ‚îú‚îÄ‚îÄ install_prometheus.yml # Metrics collection setup
‚îÇ   ‚îú‚îÄ‚îÄ install_grafana.yml    # Visualization setup
‚îÇ   ‚îú‚îÄ‚îÄ install_ipmi_exporter.yml       # Hardware monitoring
‚îÇ   ‚îú‚îÄ‚îÄ install_victoriametrics.yml     # Long-term storage
‚îÇ   ‚îî‚îÄ‚îÄ install_cluster_poweroff.yml    # Emergency response
‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îú‚îÄ‚îÄ prometheus.yml.j2      # Prometheus configuration
‚îÇ   ‚îú‚îÄ‚îÄ alertmanager.yml.j2    # Alert routing rules
‚îÇ   ‚îú‚îÄ‚îÄ alert_rules.yml.j2     # Alert definitions
‚îÇ   ‚îú‚îÄ‚îÄ endpoints/             # Service discovery configs
‚îÇ   ‚îî‚îÄ‚îÄ systemd service units
‚îî‚îÄ‚îÄ tests/
    ‚îú‚îÄ‚îÄ inventory              # Example inventory for testing
    ‚îî‚îÄ‚îÄ test.yml              # Comprehensive test playbook
```

## üß™ Testing

### Run Tests

```bash
# Full test suite
ansible-playbook -i tests/inventory tests/test.yml

# Check mode (dry run)
ansible-playbook -i tests/inventory tests/test.yml --check

# Test specific components
ansible-playbook -i tests/inventory tests/test.yml --tags monitoring
```

### Manual Verification

```bash
# Check service status
systemctl status docker.prometheus.service
systemctl status victoriametrics.service

# Test metrics endpoints
curl http://localhost:9100/metrics  # Node exporter
curl http://localhost:9400/metrics  # DCGM exporter
curl http://localhost:9090/-/healthy  # Prometheus

# Test emergency webhook
curl -X POST http://localhost:5001/alert?token=YOUR_TOKEN \
  -H "Content-Type: application/json" \
  -d '{"alerts":[{"status":"firing","labels":{"alertname":"CLUSTER-POWEROFF"}}]}'
```

## üìã Requirements

### System Requirements

- **OS**: Ubuntu 22.04 LTS or 24.04 LTS
- **Ansible**: >= 2.13
- **Docker**: >= 20.10
- **Python**: >= 3.8

### Hardware Requirements

- **NVIDIA drivers and CUDA toolkit** (for GPU nodes)
- **Slurm workload manager** (configured)
- **InfiniBand drivers** (for UFM nodes)
- **Pacemaker cluster** (for UFM HA setup)
- **IPMI/BMC access** (for hardware monitoring)

### Network Requirements

- **Open ports** between monitoring server and all nodes
- **SSH key access** for emergency poweroff
- **IPMI network access** for hardware monitoring

## üîí Security

### Sensitive Data

Always use Ansible Vault for:

```yaml
# group_vars/all/vault.yml
vault_telegram_bot_token: "ENCRYPTED"
vault_ipmi_password: "ENCRYPTED"
vault_cluster_poweroff_token: "ENCRYPTED"
vault_smtp_password: "ENCRYPTED"
```

### Network Security

- Configure firewall rules for monitoring ports
- Use dedicated monitoring network if possible
- Secure IPMI access with strong credentials
- Review webhook tokens and authentication

## üÜò Troubleshooting

### Common Issues

**Services not starting:**
```bash
# Check Docker status
systemctl status docker
journalctl -u docker.prometheus.service

# Check permissions
ls -la /etc/prometheus/
ls -la /var/lib/grafana/
```

**Missing metrics:**
```bash
# Test connectivity
telnet compute-node 9100
curl http://compute-node:9100/metrics

# Check Prometheus targets
curl http://localhost:9090/api/v1/targets
```

**UFM/Pacemaker issues:**
```bash
# Check cluster status
pcs status
pcs quorum status

# Check resource status
pcs resource show iblinkinfo-timer
```

**Emergency poweroff not working:**
```bash
# Check webhook logs
journalctl -u alert_webhook.service
tail -f /var/log/cluster_shutdown.log

# Test authentication
ssh master@compute-node 'sudo poweroff'
ipmitool -I lanplus -H bmc-host -U user -P pass chassis power status
```

### Performance Tuning

**High memory usage:**
```yaml
# Reduce retention or adjust VictoriaMetrics memory
prometheus_retention: "15d"
victoriametrics_memory_percent: 40
```

**Slow scraping:**
```yaml
# Increase intervals
prometheus_scrape_interval: "30s"
gpu_process_interval: "60s"
```

## üìû Support

For issues and feature requests:

1. Check the [troubleshooting guide](#-troubleshooting)
2. Review logs: `journalctl -u service-name`
3. Contact the SA1 GPU Cluster team
4. Submit issues to the repository


## üôè Credits

- Based on [NVIDIA DeepOps](https://github.com/NVIDIA/deepops)
- Uses [Prometheus](https://prometheus.io/) ecosystem
- Inspired by HPC monitoring best practices